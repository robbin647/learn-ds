{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3481 Assignment 2 - Random forest, Na√ève Bayesian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset\n",
    "This project uses the [UCI Forest Dataset](https://archive.ics.uci.edu/ml/datasets/Forest+type+mapping). I have downloaded the training dataset and the test dataset as \"training.csv\" and \"testing.csv\". These two csv files must be in the same directory as this notebook.  \n",
    " \n",
    "List of sklearn modules:\n",
    "- ensemble: sklearn.ensemble.RandomForestClassifier is a class for constructing random forests.\n",
    "- preprocessing: sklearn.preprocessing.LabelEncoder is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from csv the training data set\n",
    "fr_train = dftrain = pd.read_csv(\"training.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing - Convert alphabetic labels into numerics\n",
    "le = LabelEncoder()\n",
    "training_labels = fr_train.iloc[:, 0]\n",
    "labels = le.fit_transform(training_labels)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lclass_names: Index(['b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9',\n",
      "       'pred_minus_obs_H_b1', 'pred_minus_obs_H_b2', 'pred_minus_obs_H_b3',\n",
      "       'pred_minus_obs_H_b4', 'pred_minus_obs_H_b5', 'pred_minus_obs_H_b6',\n",
      "       'pred_minus_obs_H_b7', 'pred_minus_obs_H_b8', 'pred_minus_obs_H_b9',\n",
      "       'pred_minus_obs_S_b1', 'pred_minus_obs_S_b2', 'pred_minus_obs_S_b3',\n",
      "       'pred_minus_obs_S_b4', 'pred_minus_obs_S_b5', 'pred_minus_obs_S_b6',\n",
      "       'pred_minus_obs_S_b7', 'pred_minus_obs_S_b8', 'pred_minus_obs_S_b9'],\n",
      "      dtype='object')\n",
      "lclass_names: ['d ' 'h ' 'o ' 's ']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "take a look at the training dataset\n",
    "'''\n",
    "\n",
    "# list of feature names\n",
    "lfeature_names = fr_train.columns[1:]\n",
    "print(f\"lclass_names: {lfeature_names}\")\n",
    "\n",
    "# list of classes ['d ' 'h ' 'o ' 's ']\n",
    "lclass_names = le.classes_\n",
    "print(f\"lclass_names: {lclass_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first Random Forest\n",
    "RF = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=2323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "clf1 = RF.fit(fr_train.iloc[:,1:], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for first forest: \n",
      " 0.8123076923076923\n",
      "[[ 78   0   7  20]\n",
      " [  0  29   0   9]\n",
      " [ 10   0  34   2]\n",
      " [  4   9   0 123]]\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "fr_test = pd.read_csv(\"testing.csv\", sep = ',')\n",
    "testdata = fr_test.iloc[:, 1:].values # .values: just to remove the column headers in pandas DF, so as to save a runtime warning\n",
    "fr_predictions = clf1.predict(testdata)\n",
    "\n",
    "#print(lclass_names[fr_predictions])\n",
    "\n",
    "# Prediction performance\n",
    "clf1_score = accuracy_score(fr_test.iloc[:,0], lclass_names[fr_predictions])\n",
    "print(\"Accuracy score for first forest: \\n\", clf1_score)\n",
    "# See the Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[fr_predictions], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first component tree of the first Random Forest\n",
    "clf1_first_tree = clf1.estimators_[0]\n",
    "dot_data = tree.export_graphviz(clf1_first_tree, out_file=None,feature_names=lfeature_names,class_names=lclass_names,filled=True, rounded=True,special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "filepath = graph.render('RandFrst_clf1_tree1', format='png')\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (a)\n",
    "Build another three Random Forests, with different input parameters to the class constructor. And compare their performance with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for second forest: \n",
      " 0.8184615384615385\n",
      "[[ 81   0   6  18]\n",
      " [  0  28   0  10]\n",
      " [ 11   0  34   1]\n",
      " [  5   8   0 123]]\n"
     ]
    }
   ],
   "source": [
    "# second Random Forest\n",
    "RF2 = RandomForestClassifier(n_estimators=15, max_depth=3, random_state=2323)\n",
    "clf2 = RF2.fit(fr_train.iloc[:,1:], labels)\n",
    "fr_predictions2 = clf2.predict(testdata)\n",
    "# Prediction performance\n",
    "clf2_score = accuracy_score(fr_test.iloc[:,0], lclass_names[fr_predictions2])\n",
    "print(\"Accuracy score for second forest: \\n\", clf2_score)\n",
    "# Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[fr_predictions2], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for this forest: \n",
      " 0.8092307692307692\n",
      "[[ 77   0   9  19]\n",
      " [  0  30   0   8]\n",
      " [ 11   0  33   2]\n",
      " [  6   7   0 123]]\n"
     ]
    }
   ],
   "source": [
    "# third Random Forest \n",
    "RF3 = RandomForestClassifier(n_estimators=25, max_depth=3, random_state=2323)\n",
    "clf3 = RF3.fit(fr_train.iloc[:,1:], labels)\n",
    "fr_predictions3 = clf3.predict(testdata)\n",
    "# Prediction performance\n",
    "clf3_score = accuracy_score(fr_test.iloc[:,0], lclass_names[fr_predictions3])\n",
    "print(\"Accuracy score for this forest: \\n\", clf3_score)\n",
    "# Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[fr_predictions3], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for this forest: \n",
      " 0.8\n",
      "[[ 75   0  10  20]\n",
      " [  0  29   0   9]\n",
      " [ 11   0  33   2]\n",
      " [  6   7   0 123]]\n"
     ]
    }
   ],
   "source": [
    "# Fourth Random Forest \n",
    "RF4 = RandomForestClassifier(n_estimators=30, max_depth=3, random_state=2323)\n",
    "clf4 = RF4.fit(fr_train.iloc[:,1:], labels)\n",
    "fr_test = pd.read_csv(\"testing.csv\", sep = ',')\n",
    "testdata = fr_test.iloc[:, 1:]\n",
    "fr_predictions4 = clf4.predict(testdata)\n",
    "# Prediction performance\n",
    "clf4_score = accuracy_score(fr_test.iloc[:,0], lclass_names[fr_predictions4])\n",
    "print(\"Accuracy score for this forest: \\n\", clf4_score)\n",
    "# Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[fr_predictions4], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second random forest (n_estimators=15) has the best accuracy score. Its component trees will be analyzed in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the component trees is 15 \n"
     ]
    }
   ],
   "source": [
    "# look at component trees of the second forest\n",
    "print(\"The length of the component trees is %s \" % len(clf2.estimators_))\n",
    "#clf2_first_tree = clf2.estimators_[0]\n",
    "# dot_data = tree.export_graphviz(clf2_first_tree, out_file=None,feature_names=lfeature_names,class_names=lclass_names,filled=True, rounded=True,special_characters=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# filepath = graph.render('RandFrst_clf1_tree1', format='png')\n",
    "# print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the third component tree: \n",
      " 0.7507692307692307\n",
      "[[ 65   1  14  25]\n",
      " [  0  27   0  11]\n",
      " [  8   1  35   2]\n",
      " [  2  16   1 117]]\n"
     ]
    }
   ],
   "source": [
    "# try on the 3rd component tree\n",
    "clf2_third_tree = clf2.estimators_[2]\n",
    "third_tree_prediction = clf2_third_tree.predict(testdata)\n",
    "# Prediction performance\n",
    "third_tree_score = accuracy_score(fr_test.iloc[:,0], lclass_names[np.int64(third_tree_prediction)])\n",
    "print(\"Accuracy score for the third component tree: \\n\", third_tree_score)\n",
    "# See the Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[np.int64(third_tree_prediction)], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.16315059 0.         0.32864118 0.         0.34942963 0.\n",
      " 0.         0.01444022 0.14433838 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "clf2_third_tree = clf2.estimators_[2]\n",
    "print(clf2_third_tree.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the sixth component tree: \n",
      " 0.7876923076923077\n",
      "[[ 74   2   7  22]\n",
      " [  0  25   0  13]\n",
      " [ 11   1  34   0]\n",
      " [  1  12   0 123]]\n"
     ]
    }
   ],
   "source": [
    "# try on the 6th component tree\n",
    "clf2_sixth_tree = clf2.estimators_[5]\n",
    "sixth_tree_prediction = clf2_sixth_tree.predict(testdata)\n",
    "# Prediction performance\n",
    "sixth_tree_score = accuracy_score(fr_test.iloc[:,0], lclass_names[np.int64(sixth_tree_prediction)])\n",
    "print(\"Accuracy score for the sixth component tree: \\n\", sixth_tree_score)\n",
    "# See the Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[np.int64(sixth_tree_prediction)], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the ninth component tree: \n",
      " 0.7876923076923077\n",
      "[[ 79   0   2  24]\n",
      " [  1  28   0   9]\n",
      " [ 12   0  32   2]\n",
      " [ 13   6   0 117]]\n"
     ]
    }
   ],
   "source": [
    "# try on the 9th component tree\n",
    "clf2_ninth_tree = clf2.estimators_[8]\n",
    "ninth_tree_prediction = clf2_ninth_tree.predict(testdata)\n",
    "# Prediction performance\n",
    "ninth_tree_score = accuracy_score(fr_test.iloc[:,0], lclass_names[np.int64(ninth_tree_prediction)])\n",
    "print(\"Accuracy score for the ninth component tree: \\n\", ninth_tree_score)\n",
    "# See the Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[np.int64(ninth_tree_prediction)], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the twelfth component tree: \n",
      " 0.7723076923076924\n",
      "[[ 77   4   4  20]\n",
      " [  0  29   0   9]\n",
      " [ 13   0  30   3]\n",
      " [  7  14   0 115]]\n"
     ]
    }
   ],
   "source": [
    "# try on the 12th component tree\n",
    "clf2_twelfth_tree = clf2.estimators_[11]\n",
    "twelfth_tree_prediction = clf2_twelfth_tree.predict(testdata)\n",
    "# Prediction performance\n",
    "twelfth_tree_score = accuracy_score(fr_test.iloc[:,0], lclass_names[np.int64(twelfth_tree_prediction)])\n",
    "print(\"Accuracy score for the twelfth component tree: \\n\", twelfth_tree_score)\n",
    "# See the Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[np.int64(twelfth_tree_prediction)], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the fifteenth component tree: \n",
      " 0.7846153846153846\n",
      "[[ 79   2   2  22]\n",
      " [  1  30   0   7]\n",
      " [ 12   0  33   1]\n",
      " [ 11  12   0 113]]\n"
     ]
    }
   ],
   "source": [
    "# try on the 15th component tree\n",
    "clf2_fifteenth_tree = clf2.estimators_[14]\n",
    "fifteenth_tree_prediction = clf2_fifteenth_tree.predict(testdata)\n",
    "# Prediction performance\n",
    "fifteenth_tree_score = accuracy_score(fr_test.iloc[:,0], lclass_names[np.int64(fifteenth_tree_prediction)])\n",
    "print(\"Accuracy score for the fifteenth component tree: \\n\", fifteenth_tree_score)\n",
    "# See the Confusion Matrix for this tree\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[np.int64(fifteenth_tree_prediction)], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if any component tree has better performance than the forest\n",
    "test_set_labels_encoded = le.transform(fr_test.iloc[:,0])\n",
    "testdata_size = testdata.shape[0]\n",
    "for i in range (15):\n",
    "    ith_tree = clf2.estimators_[i]\n",
    "    ith_tree_prediction = ith_tree.predict(testdata)\n",
    "    print(np.round(accuracy_score(fr_test.iloc[:,0], lclass_names[np.int64(ith_tree_prediction)]), 3))\n",
    "    # print(ith_tree_prediction[112:124].ravel())\n",
    "    # is_same_or_not = test_set_labels_encoded ^ np.int64(ith_tree_prediction) # if prediction == label, XOR will give 0\n",
    "    # num_of_misclassification = len(is_same_or_not.nonzero())\n",
    "    # ith_tree_accuracy_score = (testdata_size - num_of_misclassification)  / testdata_size * 1.0\n",
    "    # print(ith_tree_accuracy_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (c)\n",
    "Find out all ```feature_importances_[]``` values of the selected trees in Task (b).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b7' 'b9' 'pred_minus_obs_H_b2' 'pred_minus_obs_H_b5'\n",
      " 'pred_minus_obs_H_b6']\n",
      "['b7' 'b9' 'pred_minus_obs_H_b2' 'pred_minus_obs_H_b5'\n",
      " 'pred_minus_obs_H_b6']\n",
      "['pred_minus_obs_H_b1' 'pred_minus_obs_H_b2' 'pred_minus_obs_H_b7'\n",
      " 'pred_minus_obs_H_b8' 'pred_minus_obs_H_b9' 'pred_minus_obs_S_b7']\n",
      "['b3' 'b4' 'b6' 'pred_minus_obs_H_b2']\n",
      "['b1' 'b4' 'b5' 'b9' 'pred_minus_obs_H_b8' 'pred_minus_obs_S_b5']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array('b1,b2,b3,b4,b5,b6,b7,b8,b9,pred_minus_obs_H_b1,pred_minus_obs_H_b2,pred_minus_obs_H_b3,pred_minus_obs_H_b4,pred_minus_obs_H_b5,pred_minus_obs_H_b6,pred_minus_obs_H_b7,pred_minus_obs_H_b8,pred_minus_obs_H_b9,pred_minus_obs_S_b1,pred_minus_obs_S_b2,pred_minus_obs_S_b3,pred_minus_obs_S_b4,pred_minus_obs_S_b5,pred_minus_obs_S_b6,pred_minus_obs_S_b7,pred_minus_obs_S_b8,pred_minus_obs_S_b9'.split(','))\n",
    "print(feature_names[clf2_third_tree.feature_importances_.nonzero()])\n",
    "print(feature_names[clf2_sixth_tree.feature_importances_.nonzero()])\n",
    "print(feature_names[clf2_ninth_tree.feature_importances_.nonzero()])\n",
    "print(feature_names[clf2_twelfth_tree.feature_importances_.nonzero()])\n",
    "print(feature_names[clf2_fifteenth_tree.feature_importances_.nonzero()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.16315059 0.         0.32864118 0.         0.34942963 0.\n",
      " 0.         0.01444022 0.14433838 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.24076416 0.         0.35391447 0.         0.06891281 0.\n",
      " 0.         0.32011285 0.01629571 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34855784 0.33227218 0.\n",
      " 0.         0.         0.         0.01963593 0.25909322 0.01354952\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02689132 0.         0.        ]\n",
      "[0.         0.         0.35709851 0.06657728 0.         0.24605302\n",
      " 0.         0.         0.         0.         0.33027119 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[0.0150263  0.         0.         0.29864451 0.31199232 0.\n",
      " 0.         0.         0.01467625 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.34459329 0.\n",
      " 0.         0.         0.         0.         0.01506733 0.\n",
      " 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(clf2_third_tree.feature_importances_)\n",
    "print(clf2_sixth_tree.feature_importances_)\n",
    "print(clf2_ninth_tree.feature_importances_)\n",
    "print(clf2_twelfth_tree.feature_importances_)\n",
    "print(clf2_fifteenth_tree.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (d)\n",
    "Na√ève Bayesian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a NB classifier \n",
    "nb = GaussianNB()\n",
    "# Train the model\n",
    "nb.fit(fr_train.iloc[:,1:], labels)\n",
    "# Test model on test set\n",
    "testdata = fr_test.iloc[:, 1:]\n",
    "NB_prediction = nb.predict(testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB accuracy score: 0.803077\n"
     ]
    }
   ],
   "source": [
    "# print(NB_prediction[:10])\n",
    "NB_accuracy_score = accuracy_score(fr_test.iloc[:,0], lclass_names[NB_prediction])\n",
    "print(\"NB accuracy score: %f\" % NB_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 81   0  11  13]\n",
      " [  0  30   0   8]\n",
      " [  8   0  37   1]\n",
      " [ 11  12   0 113]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(fr_test.iloc[:,0], lclass_names[NB_prediction], labels=['d ', 'h ', 'o ', 's ']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd9d2118eb93ddfbfd8479d5fdd1bd6d981f2024a3aa868972e2e3c5dd26fd3e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
